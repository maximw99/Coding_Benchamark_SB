{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT7BNBPDsqS4"
      },
      "source": [
        "\n",
        "\n",
        "# Pipeline to produce llm code samples based on \"Human-Eval\" dataset of opeanai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrqRczzCsqMy"
      },
      "source": [
        "## Pip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWT24ObO5TXz"
      },
      "source": [
        "### Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z1si7uaL5Wb8"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3274864288.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    git clone \"https://github.com/openai/human-eval.git\"\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Human-Eval by OpenAi for the problem dataset\n",
        "!git clone \"https://github.com/openai/human-eval.git\"\n",
        "\n",
        "# Accelerate for gpu computing\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riW3SYJE5Y1M"
      },
      "source": [
        "### GPTQ model requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1--A7lPds524",
        "outputId": "feec7a1d-1dc1-4c50-f2d6-6abf6197f57e"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1134316694.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ \n",
        "!pip install optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKhpLByw50SH"
      },
      "source": [
        "### Phi2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMrPaFmh58yN"
      },
      "outputs": [],
      "source": [
        "!pip install einops\n",
        "!pip install auto-gptq\n",
        "!pip install optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSgb9CO-54zc"
      },
      "source": [
        "### Wizard-Coder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrlHUyv459Ne"
      },
      "outputs": [],
      "source": [
        "!pip install auto_gptq\n",
        "!pip install optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkQraGOdszbk"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flaMXjVG6Un5"
      },
      "source": [
        "### Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY3BQ6bC6nvn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "import time\n",
        "import sys\n",
        "sys.path.insert(0,'/content/human-eval')\n",
        "from human_eval.data import write_jsonl, read_problems\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubEeE-S56Ut8"
      },
      "source": [
        "### Wizardcoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5Wj_f7i6ut5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, pipeline, logging\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFvUWNUjwVgI"
      },
      "source": [
        "## Model setup\n",
        "\n",
        "All Models are quantized version of original models and used from \"https://huggingface.co/TheBloke\"\n",
        "\n",
        "The models are lisensed by their original owners\n",
        "\n",
        "Codellama: Meta (https://huggingface.co/codellama)\n",
        "WizardCoder: WizardLM (https://huggingface.co/WizardLM)\n",
        "Phi-2: Microsoft (https://huggingface.co/microsoft/phi-2)\n",
        "\n",
        "What is GPTQ? - GPTQ is a post-training quantization method to compress LLMs and obtain nearly similiar accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3EiJNzq7FyF"
      },
      "source": [
        "### Codellama-13b (GPTQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUsVo8JP7OBm"
      },
      "outputs": [],
      "source": [
        "model_path = \"TheBloke/CodeLlama-13B-Instruct-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model,device_map=\"auto\", trust_remote_code=True, revision=\"gptq-8bit-128g-actorder_True\") # Biggest gptq model for codellama 13b\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/CodeLlama-13B-Instruct-GPTQ\", use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqfXocDN7F3v"
      },
      "source": [
        "### Codellama-34b (GPTQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx2S-kkG7OYn"
      },
      "outputs": [],
      "source": [
        "model_path = \"TheBloke/Phind-CodeLlama-34B-v2-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model,device_map=\"auto\", trust_remote_code=True, revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Phind-CodeLlama-34B-v2-GPTQ\", use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrEbkIF67F9k"
      },
      "source": [
        "### Wizard-Coder-13b (GPTQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxZ4XDD3A4Xg"
      },
      "outputs": [],
      "source": [
        "model_path = \"TheBloke/WizardCoder-Python-13B-V1.0-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1sdf6-TA9iI"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFDf3U9O7GDL"
      },
      "source": [
        "### Wizard-Coder-15b (GPTQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXGjLPj0AzGD"
      },
      "outputs": [],
      "source": [
        "model_path = \"TheBloke/WizardCoder-15B-1.0-GPTQ\"\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "        use_safetensors=True,\n",
        "        device=\"cuda:0\",\n",
        "        use_triton=False,\n",
        "        quantize_config=None)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRTNJ_CF-NS3"
      },
      "source": [
        "### Phi-2 (GPTQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDBB9ri8Aps0"
      },
      "outputs": [],
      "source": [
        "model_path = \"TheBloke/phi-2-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=True,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmkerDxIb9Q1"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHwn-NF8Bi4k"
      },
      "source": [
        "### Basics\n",
        "\n",
        "Basic functions to create the dataset for working with a LLM code generation testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVe9Zk1i5qup"
      },
      "outputs": [],
      "source": [
        "def generate_samples_pass1(problems):\n",
        "\n",
        "  # Store samples\n",
        "  samples = []\n",
        "  n = 0\n",
        "\n",
        "  # For every problem in dataset\n",
        "  for task_id, problem in problems.items():\n",
        "    answer = dict(task_id = task_id, completion = generate_solution_llama13b(problem[\"prompt\"])) # change model name for different model\n",
        "    samples.append(answer)\n",
        "    print(\"step: \", n, \"from 163 done\")\n",
        "    n = n+1\n",
        "\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i84AVC5BtNJ"
      },
      "outputs": [],
      "source": [
        "def generate_samples_pass10(problems):\n",
        "\n",
        "  # Store samples\n",
        "  samples_1 = []\n",
        "  samples_10 = []\n",
        "  n = 0\n",
        "  m = 0\n",
        "  examples = 1\n",
        "\n",
        "  # For every problem in dataset\n",
        "  for task_id, problem in problems.items():\n",
        "    res = dict(task_id = task_id, completion = generate_solution_llama13b_tmplow(problem[\"prompt\"])) # change model name for different model\n",
        "    samples_1.append(res)\n",
        "    for n in range(n, examples):\n",
        "      answer = dict(task_id = task_id, completion = generate_solution_llama13b_tmphigh(problem[\"prompt\"])) # change model name for different model\n",
        "      samples_10.append(answer)\n",
        "      print(\"step: \", n, \"done\")\n",
        "      n = n+1\n",
        "    n = 0\n",
        "    print(\"block done: \", m, \"of 163\")\n",
        "    m = m+1\n",
        "\n",
        "  return (samples_1, samples_10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKh256k1C-_x"
      },
      "source": [
        "### get_code funtions\n",
        "\n",
        "about: The get_code function family is a group of functions I created to get the fitting result out of every model used.\n",
        "        My goal was to just get the code as result without prompt or additional components like test or comments.\n",
        "\n",
        "note: For know just codellama functions are finished and fully useable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg07QgBoD63Q"
      },
      "outputs": [],
      "source": [
        "def get_code_llama13b(text: str):\n",
        "\n",
        "  # Find the index where \"def \" starts\n",
        "  begin_index = text.find('def ', 200)\n",
        "  if begin_index == -1:\n",
        "      return \"\"  # return an empty string if \"def \" is not found\n",
        "\n",
        "  # Extract the code starting from \"def \"\n",
        "  function = text[begin_index:]\n",
        "\n",
        "  # Find the index of \"```\" that comes after \"def \"\n",
        "  end_code_index = function.find('```')\n",
        "\n",
        "  # If \"```\" is found, return the code until just before \"```\"\n",
        "  if end_code_index != -1:\n",
        "      code = function[:end_code_index]\n",
        "  else:\n",
        "      # If no \"```\" is found, return the code until the end of the string\n",
        "      code = function\n",
        "\n",
        "  # Remove prompt if existing\n",
        "  if code.find('\"\"\"'):\n",
        "    promptbegin_index = code.find('\"\"\"')\n",
        "    #print(promptbegin_index)\n",
        "    code_del = code[promptbegin_index:]\n",
        "    promptend_index = code_del.find('\"\"\"', 10)\n",
        "    print(promptend_index)\n",
        "    code_del = code_del[:promptend_index]\n",
        "\n",
        "    print(code_del)\n",
        "\n",
        "  # Removing leading and trailing whitespaces and newlines\n",
        "  return code.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2ut2QosrDj6m"
      },
      "outputs": [],
      "source": [
        "def get_code_wizard15b(text: str):\n",
        "\n",
        "  # Find the index where \"def \" starts\n",
        "  def_index = text.find('def ', 300)\n",
        "  if def_index == -1:\n",
        "      return \"\"  # return an empty string if \"def \" is not found\n",
        "\n",
        "  # Extract the code starting from \"def \"\n",
        "  code_after_def = text[def_index:]\n",
        "\n",
        "  # Find the index of \"```\" that comes after \"def \"\n",
        "  end_code_index = code_after_def.find('```')\n",
        "\n",
        "  # If \"```\" is found, return the code until just before \"```\"\n",
        "  if end_code_index != -1:\n",
        "      code = code_after_def[:end_code_index]\n",
        "  else:\n",
        "      # If no \"```\" is found, return the code until the end of the string\n",
        "      code = code_after_def\n",
        "\n",
        "  # Removing leading and trailing whitespaces and newlines\n",
        "  return code.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8esmpyKnDvXl"
      },
      "outputs": [],
      "source": [
        "def get_code_wizard13b(text: str):\n",
        "\n",
        "  # Find the index where \"def \" starts\n",
        "  def_index = text.find('def ', 200)\n",
        "  if def_index == -1:\n",
        "      return \"\"  # return an empty string if \"def \" is not found\n",
        "\n",
        "  # Extract the code starting from \"def \"\n",
        "  code_after_def = text[def_index:]\n",
        "\n",
        "  # Find the index of \"```\" that comes after \"def \"\n",
        "  end_code_index = code_after_def.find('# Test')\n",
        "\n",
        "  # If \"```\" is found, return the code until just before \"```\"\n",
        "  if end_code_index != -1:\n",
        "      code = code_after_def[:end_code_index]\n",
        "  else:\n",
        "      # If no \"```\" is found, return the code until the end of the string\n",
        "      code = code_after_def\n",
        "\n",
        "  # Removing leading and trailing whitespaces and newlines\n",
        "  return code.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_wpCxIeEA10"
      },
      "outputs": [],
      "source": [
        "def get_code_phi2(text: str):\n",
        "\n",
        "  # Find the index where \"def \" starts\n",
        "  def_index = text.find('def ')\n",
        "  if def_index == -1:\n",
        "      return \"\"  # return an empty string if \"def \" is not found\n",
        "\n",
        "  # Extract the code starting from \"def \"\n",
        "  code_after_def = text[def_index:]\n",
        "\n",
        "  # Find the index of \"```\" that comes after \"def \"\n",
        "  end_code_index = code_after_def.find('if __name__')\n",
        "\n",
        "  # If \"```\" is found, return the code until just before \"```\"\n",
        "  if end_code_index != -1:\n",
        "      code = code_after_def[:end_code_index]\n",
        "  else:\n",
        "      # If no \"```\" is found, return the code until the end of the string\n",
        "      code = code_after_def\n",
        "\n",
        "  # Removing leading and trailing whitespaces and newlines\n",
        "  return code.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### generate_solution functions\n",
        "\n",
        "about: The generate_solution function family is a group of functions I created to get the fitting result out of every model used.\n",
        "        My functions take \"Human Eval\" suggested funtion as build since I will primarily use their test for my research\n",
        "\n",
        "Credit: \"https://github.com/openai/human-eval\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6oj6yfSCeOs"
      },
      "outputs": [],
      "source": [
        "def generate_solution_llama13b_tmplow(prompt: str):\n",
        "\n",
        "  # Prompt tempate for code llama models\n",
        "  prompt_template=f'''[INST] Write Python code to solve the following coding problem that obeys the constraints and passes the example test cases. You must end your code using ```:\n",
        "  {prompt}\n",
        "  [/INST]\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Prepare input\n",
        "  input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "\n",
        "  # Load input to model\n",
        "  output = model.generate(inputs=input_ids, temperature=0.1, do_sample=True, top_p=0.95, max_new_tokens=512)\n",
        "\n",
        "  # Decode result\n",
        "  code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  # Get only \"useable\" code\n",
        "  solution = get_code(code)\n",
        "\n",
        "  #return code\n",
        "  return solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_solution_llama13b_tmphigh(prompt: str):\n",
        "\n",
        "  # Prompt tempate for code llama models\n",
        "  prompt_template=f'''[INST] Write Python code to solve the following coding problem that obeys the constraints and passes the example test cases. You must end your code using ```:\n",
        "  {prompt}\n",
        "  [/INST]\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Prepare input\n",
        "  input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "\n",
        "  # Load input to model\n",
        "  output = model.generate(inputs=input_ids, temperature=1, do_sample=True, top_p=0.95, max_new_tokens=512)\n",
        "\n",
        "  # Decode result\n",
        "  code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  # Get only \"useable\" code\n",
        "  solution = get_code(code)\n",
        "\n",
        "  #return code\n",
        "  return solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq--wvCADdK6"
      },
      "outputs": [],
      "source": [
        "def generate_solution_wizard15b(prompt: str):\n",
        "\n",
        "  # Prompt\n",
        "  prompt_template = '''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "  ### Instruction: {prompt}\n",
        "\n",
        "  ### Response:'''\n",
        "\n",
        "  # Prombt format\n",
        "  prompt = prompt_template.format(prompt=prompt)\n",
        "\n",
        "\n",
        "  # LLM generation\n",
        "  outputs = pipe(prompt, max_new_tokens=1024, do_sample=True, temperature=0.1, top_k=50, top_p=0.95)\n",
        "\n",
        "\n",
        "  # Fix output\n",
        "  code = get_code(outputs[0]['generated_text'])\n",
        "\n",
        "\n",
        "  return code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXKU9bLpDqSw"
      },
      "outputs": [],
      "source": [
        "def generate_solution_wizard13b(prompt: str):\n",
        "\n",
        "  # Prompt\n",
        "  prompt_template=f'''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "  ### Instruction:\n",
        "  {prompt}\n",
        "\n",
        "  ### Response:\n",
        "\n",
        "  '''\n",
        "\n",
        "\n",
        "  # LLM generation\n",
        "  input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "  output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id,)\n",
        "\n",
        "\n",
        "  # Fix output\n",
        "  code = get_code(tokenizer.decode(output[0]))\n",
        "\n",
        "\n",
        "  #return code\n",
        "  return code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_aePTEDD8iI"
      },
      "outputs": [],
      "source": [
        "def generate_solution_phi2(prompt: str):\n",
        "\n",
        "  # Prompt\n",
        "  #system_message = \"You are a code writing assistant.\"\n",
        "\n",
        "  # Prompt\n",
        "  pro=f'''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "  ### Instruction:\n",
        "  {prompt}\n",
        "\n",
        "  ### Response:\n",
        "\n",
        "  '''\n",
        "\n",
        "  prompt_template=f'''Instruct: {prompt}\n",
        "  Output:\n",
        "  '''\n",
        "\n",
        "  # LLM generation\n",
        "  input_ids = tokenizer(pro, return_tensors='pt').input_ids.cuda()\n",
        "  output = model.generate(inputs=input_ids, temperature=0.2, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
        "\n",
        "\n",
        "  # Fix output\n",
        "  code = get_code(tokenizer.decode(output[0]))\n",
        "\n",
        "\n",
        "  return code\n",
        "  #return tokenizer.decode(output[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtNFXEdUb_SZ"
      },
      "source": [
        "## Calls\n",
        "\n",
        "The actuall pipeline of calls to produce a dataset for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vm5yubxr6Y2",
        "outputId": "508cff6e-6c67-422f-8cb1-95329403f642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step:  0 from 163 done\n",
            "step:  1 from 163 done\n",
            "step:  2 from 163 done\n",
            "step:  3 from 163 done\n",
            "step:  4 from 163 done\n",
            "step:  5 from 163 done\n",
            "step:  6 from 163 done\n",
            "step:  7 from 163 done\n",
            "step:  8 from 163 done\n",
            "step:  9 from 163 done\n",
            "step:  10 from 163 done\n",
            "step:  11 from 163 done\n",
            "step:  12 from 163 done\n",
            "step:  13 from 163 done\n",
            "step:  14 from 163 done\n",
            "step:  15 from 163 done\n",
            "step:  16 from 163 done\n",
            "step:  17 from 163 done\n",
            "step:  18 from 163 done\n",
            "step:  19 from 163 done\n",
            "step:  20 from 163 done\n",
            "step:  21 from 163 done\n",
            "step:  22 from 163 done\n",
            "step:  23 from 163 done\n",
            "step:  24 from 163 done\n",
            "step:  25 from 163 done\n",
            "step:  26 from 163 done\n",
            "step:  27 from 163 done\n",
            "step:  28 from 163 done\n",
            "step:  29 from 163 done\n",
            "step:  30 from 163 done\n",
            "step:  31 from 163 done\n",
            "step:  32 from 163 done\n",
            "step:  33 from 163 done\n",
            "step:  34 from 163 done\n",
            "step:  35 from 163 done\n",
            "step:  36 from 163 done\n",
            "step:  37 from 163 done\n",
            "step:  38 from 163 done\n",
            "step:  39 from 163 done\n",
            "step:  40 from 163 done\n",
            "step:  41 from 163 done\n",
            "step:  42 from 163 done\n",
            "step:  43 from 163 done\n",
            "step:  44 from 163 done\n",
            "step:  45 from 163 done\n",
            "step:  46 from 163 done\n",
            "step:  47 from 163 done\n",
            "step:  48 from 163 done\n",
            "step:  49 from 163 done\n",
            "step:  50 from 163 done\n",
            "step:  51 from 163 done\n",
            "step:  52 from 163 done\n",
            "step:  53 from 163 done\n",
            "step:  54 from 163 done\n",
            "step:  55 from 163 done\n",
            "step:  56 from 163 done\n",
            "step:  57 from 163 done\n",
            "step:  58 from 163 done\n",
            "step:  59 from 163 done\n",
            "step:  60 from 163 done\n",
            "step:  61 from 163 done\n",
            "step:  62 from 163 done\n",
            "step:  63 from 163 done\n",
            "step:  64 from 163 done\n",
            "step:  65 from 163 done\n",
            "step:  66 from 163 done\n",
            "step:  67 from 163 done\n",
            "step:  68 from 163 done\n",
            "step:  69 from 163 done\n",
            "step:  70 from 163 done\n",
            "step:  71 from 163 done\n",
            "step:  72 from 163 done\n",
            "step:  73 from 163 done\n",
            "step:  74 from 163 done\n",
            "step:  75 from 163 done\n",
            "step:  76 from 163 done\n",
            "step:  77 from 163 done\n",
            "step:  78 from 163 done\n",
            "step:  79 from 163 done\n",
            "step:  80 from 163 done\n",
            "step:  81 from 163 done\n",
            "step:  82 from 163 done\n",
            "step:  83 from 163 done\n",
            "step:  84 from 163 done\n",
            "step:  85 from 163 done\n",
            "step:  86 from 163 done\n",
            "step:  87 from 163 done\n",
            "step:  88 from 163 done\n",
            "step:  89 from 163 done\n",
            "step:  90 from 163 done\n",
            "step:  91 from 163 done\n",
            "step:  92 from 163 done\n",
            "step:  93 from 163 done\n",
            "step:  94 from 163 done\n",
            "step:  95 from 163 done\n",
            "step:  96 from 163 done\n",
            "step:  97 from 163 done\n",
            "step:  98 from 163 done\n",
            "step:  99 from 163 done\n",
            "step:  100 from 163 done\n",
            "step:  101 from 163 done\n",
            "step:  102 from 163 done\n",
            "step:  103 from 163 done\n",
            "step:  104 from 163 done\n",
            "step:  105 from 163 done\n",
            "step:  106 from 163 done\n",
            "step:  107 from 163 done\n",
            "step:  108 from 163 done\n",
            "step:  109 from 163 done\n",
            "step:  110 from 163 done\n",
            "step:  111 from 163 done\n",
            "step:  112 from 163 done\n",
            "step:  113 from 163 done\n",
            "step:  114 from 163 done\n",
            "step:  115 from 163 done\n",
            "step:  116 from 163 done\n",
            "step:  117 from 163 done\n",
            "step:  118 from 163 done\n",
            "step:  119 from 163 done\n",
            "step:  120 from 163 done\n",
            "step:  121 from 163 done\n",
            "step:  122 from 163 done\n",
            "step:  123 from 163 done\n",
            "step:  124 from 163 done\n",
            "step:  125 from 163 done\n",
            "step:  126 from 163 done\n",
            "step:  127 from 163 done\n",
            "step:  128 from 163 done\n",
            "step:  129 from 163 done\n",
            "step:  130 from 163 done\n",
            "step:  131 from 163 done\n",
            "step:  132 from 163 done\n",
            "step:  133 from 163 done\n",
            "step:  134 from 163 done\n",
            "step:  135 from 163 done\n",
            "step:  136 from 163 done\n",
            "step:  137 from 163 done\n",
            "step:  138 from 163 done\n",
            "step:  139 from 163 done\n",
            "step:  140 from 163 done\n",
            "step:  141 from 163 done\n",
            "step:  142 from 163 done\n",
            "step:  143 from 163 done\n",
            "step:  144 from 163 done\n",
            "step:  145 from 163 done\n",
            "step:  146 from 163 done\n",
            "step:  147 from 163 done\n",
            "step:  148 from 163 done\n",
            "step:  149 from 163 done\n",
            "step:  150 from 163 done\n",
            "step:  151 from 163 done\n",
            "step:  152 from 163 done\n",
            "step:  153 from 163 done\n",
            "step:  154 from 163 done\n",
            "step:  155 from 163 done\n",
            "step:  156 from 163 done\n",
            "step:  157 from 163 done\n",
            "step:  158 from 163 done\n",
            "step:  159 from 163 done\n",
            "step:  160 from 163 done\n",
            "step:  161 from 163 done\n",
            "step:  162 from 163 done\n",
            "step:  163 from 163 done\n",
            "time taken to run: 2390.102982332\n"
          ]
        }
      ],
      "source": [
        "# Execute pipe\n",
        "t1 = time.perf_counter()\n",
        "problems = read_problems()\n",
        "result = generate_samples_pass10(problems)\n",
        "json_1 = result[1]\n",
        "json_10 = result[10]\n",
        "write_jsonl(\"LLM_gen_1.jsonl\", json_1)\n",
        "write_jsonl(\"LLM_gen_10.jsonl\", json_10)\n",
        "t2 = time.perf_counter()\n",
        "print('time taken to run:',(t2-t1)/60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeAkBfJub19S"
      },
      "source": [
        "## Debug\n",
        "\n",
        "Debug functions for pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLrx6XTMfPrW"
      },
      "outputs": [],
      "source": [
        "# prompt = \"A function that creates an array and fills it with random numbers 1 to 20\"\n",
        "# prompt_template=f'''[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```. Please start the code with a def:\n",
        "# {prompt}\n",
        "# [/INST]\n",
        "\n",
        "# '''\n",
        "\n",
        "# print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "# input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "# output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
        "# print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-Nm-jHSUM8S"
      },
      "outputs": [],
      "source": [
        "# problems = read_problems()\n",
        "# prob = problems[\"HumanEval/1\"][\"prompt\"]\n",
        "# prob = \"Write a function that shows the first 100 prime numbers\"\n",
        "# prompt = prob\n",
        "# prompt_template=f'''[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
        "# {prompt}\n",
        "# [/INST]\n",
        "\n",
        "# '''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gixRiF9sg_Or"
      },
      "outputs": [],
      "source": [
        "# input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "# output = model.generate(inputs=input_ids, temperature=0.2, do_sample=True, top_p=0.95, max_new_tokens=512)\n",
        "# code = tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjs0DiJTpoEI",
        "outputId": "34b12d66-916c-4555-f428-7ef39a163a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
            "Write a function that shows the first 100 prime numbers\n",
            "[/INST]\n",
            "\n",
            "[PYTHON]\n",
            "def get_first_100_prime_numbers(n):\n",
            "    prime_numbers = []\n",
            "    for num in range(2, n+1):\n",
            "        if all(num % i != 0 for i in range(2, num)):\n",
            "            prime_numbers.append(num)\n",
            "    return prime_numbers[:100]\n",
            "[/PYTHON]\n",
            "\n",
            "[TESTS]\n",
            "# Test case 1:\n",
            "assert get_first_100_prime_numbers(100) == [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n",
            "# Test case 2:\n",
            "assert get_first_100_prime_numbers(1000) == [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 2\n"
          ]
        }
      ],
      "source": [
        "# print(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AenmhPoMBGYp"
      },
      "outputs": [],
      "source": [
        "# problems = read_problems()\n",
        "# prob = problems[\"HumanEval/0\"][\"prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PAzus4yBZer",
        "outputId": "15860c3e-2c62-4c3b-e2c6-a09aa2d52e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1\n",
            "\n",
            "-1\n",
            "\n",
            "--------------------------\n",
            "[INST] Write Python code to solve the following coding problem that obeys the constraints and passes the example test cases. You must end your code using ```:\n",
            "  from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "\n",
            "  [/INST]\n",
            "\n",
            "  \n",
            "\n",
            "[PYTHON]\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    for i in range(len(numbers) - 1):\n",
            "        for j in range(i + 1, len(numbers)):\n",
            "            if abs(numbers[i] - numbers[j]) <= threshold:\n",
            "                return True\n",
            "    return False\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    print(has_close_elements([1.0, 2.0, 3.0], 0.5))\n",
            "    print(has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3))\n",
            "[/PYTHON]\n",
            "\n",
            "--------------------------\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    for i in range(len(numbers) - 1):\n",
            "        for j in range(i + 1, len(numbers)):\n",
            "            if abs(numbers[i] - numbers[j]) <= threshold:\n",
            "                return True\n",
            "    return False\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    print(has_close_elements([1.0, 2.0, 3.0], 0.5))\n",
            "    print(has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3))\n",
            "[/PYTHON]\n"
          ]
        }
      ],
      "source": [
        "# res = generate_solution(prob)\n",
        "# fix = get_code(res)\n",
        "\n",
        "# print(\"--------------------------\")\n",
        "# print(res)\n",
        "# print(\"--------------------------\")\n",
        "# print(fix)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WrqRczzCsqMy",
        "VWT24ObO5TXz",
        "riW3SYJE5Y1M",
        "ZKhpLByw50SH",
        "sSgb9CO-54zc",
        "SkQraGOdszbk",
        "flaMXjVG6Un5",
        "ubEeE-S56Ut8",
        "-3EiJNzq7FyF",
        "YqfXocDN7F3v",
        "YrEbkIF67F9k",
        "tFDf3U9O7GDL",
        "YRTNJ_CF-NS3",
        "uHwn-NF8Bi4k",
        "cKh256k1C-_x",
        "2N5HbW8lDWKi",
        "q59PqHtxDWT1",
        "u2ykBVaCD2bY",
        "QtNFXEdUb_SZ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
